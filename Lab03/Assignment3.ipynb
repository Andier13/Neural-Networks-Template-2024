{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902f5dea-0226-40ea-bd2e-1798e3ea0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data1',\n",
    "        transform=lambda x: np.array(x).flatten(),\n",
    "        download=True,\n",
    "        train=is_train)\n",
    "    \n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    \n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "        \n",
    "    return mnist_data, mnist_labels\n",
    "    \n",
    "i_train_X, i_train_Y = download_mnist(True)\n",
    "i_test_X, i_test_Y = download_mnist(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f491c42f-0075-4c6a-9f44-d3f6d18286f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot_encoding(labels: np.ndarray) -> np.ndarray:\n",
    "    return np.eye(10)[labels]\n",
    "\n",
    "def normalized(input: np.ndarray) -> np.ndarray:\n",
    "    return input / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa8a9a1-e22e-416a-b358-a79feded229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_initial_data(train_X, train_Y, test_X, test_Y):\n",
    "    train_Y = convert_to_one_hot_encoding(train_Y)\n",
    "    test_Y = convert_to_one_hot_encoding(test_Y)\n",
    "    \n",
    "    train_X = np.array(train_X)\n",
    "    test_X = np.array(test_X)\n",
    "\n",
    "    train_X = normalized(train_X)\n",
    "    test_X = normalized(test_X)\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "073dd600-6f69-4743-9489-96dc74f063bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def xavier_uniform(fan_in, fan_out):\n",
    "    return math.sqrt(6/(fan_in + fan_out))\n",
    "\n",
    "def he_uniform(fan_in, fan_out):\n",
    "    return math.sqrt(6 / fan_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "17074afd-177c-4764-8c2e-db5c023f6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_and_biases(neurons_per_layer):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(1, len(neurons_per_layer)):\n",
    "        fan_in, fan_out = neurons_per_layer[i - 1], neurons_per_layer[i]\n",
    "        limit = he_uniform(fan_in, fan_out) #if i < len(neurons_per_layer) - 1 else he_uniform(fan_in, fan_out)\n",
    "        \n",
    "        weights.append(np.random.uniform(low=-limit, high=limit, size=(fan_in, fan_out)))\n",
    "        biases.append(np.zeros((1, fan_out)))\n",
    "\n",
    "    # weights = [np.random.randn(neurons_per_layer[i], neurons_per_layer[i + 1]) * np.sqrt(1.0 / neurons_per_layer[i]) for i in range(len(neurons_per_layer) - 1)]\n",
    "    # biases = [np.zeros((1, neurons_per_layer[i + 1])) for i in range(len(neurons_per_layer) - 1)]\n",
    "        \n",
    "    return weights, biases\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a980fcae-6c5a-4621-b879-54e81856273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z)) #-np.clip(z, -100, 100)\n",
    "\n",
    "def sigmoid_prime(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def tanh(z):\n",
    "    return 2*sigmoid(2*z) - 1\n",
    "\n",
    "def tanh_prime(y):\n",
    "    return 1 - y*y\n",
    "\n",
    "RELU_LEAK = 0.01\n",
    "\n",
    "def leaky_relu(z):\n",
    "    return np.where(z>0, z, z*RELU_LEAK)\n",
    "\n",
    "def leaky_relu_prime(y):\n",
    "    return np.where(y>0, 1, RELU_LEAK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ea199ffe-8386-43c4-aca0-568688e50758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return e_z / e_z.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "316bdd7b-0ef0-43ce-a100-d091449586ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "        'leaky_relu': (leaky_relu, leaky_relu_prime),\n",
    "        'sigmoid': (sigmoid, sigmoid_prime),\n",
    "        'tanh': (tanh, tanh_prime)\n",
    "    }\n",
    "\n",
    "activation_function, activation_function_prime = activation_functions['leaky_relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "826aff6e-52cc-435b-a7a0-98b935e565bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_train(inputs, weights, biases):\n",
    "    output_per_layer = [inputs]\n",
    "    DROPOUT_RATE = 0.1\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        w = weights[i]\n",
    "        b = biases[i]\n",
    "        x = output_per_layer[-1]\n",
    "        \n",
    "        z = x @ w + b\n",
    "\n",
    "        #DROPOUT\n",
    "        if i < len(weights) - 1:\n",
    "            #z *= np.random.binomial(1, 1 - DROPOUT_RATE, z.shape)\n",
    "            z *= np.random.choice([0, 1/(1-DROPOUT_RATE)], z.shape, p=[DROPOUT_RATE, 1-DROPOUT_RATE]) #[0, 1/(1-DROPOUT_RATE)]\n",
    "            \n",
    "        y = activation_function(z) if i < len(weights) - 1 else softmax(z)\n",
    "        output_per_layer.append(y)\n",
    "        \n",
    "    return output_per_layer\n",
    "\n",
    "def forward_propagation_test(inputs, weights, biases):\n",
    "    output_per_layer = [inputs]\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        w = weights[i]\n",
    "        b = biases[i]\n",
    "        x = output_per_layer[-1]\n",
    "        \n",
    "        z = x @ w + b\n",
    "        \n",
    "        y = activation_function(z) if i < len(weights) - 1 else softmax(z)\n",
    "        output_per_layer.append(y)\n",
    "        \n",
    "    return output_per_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dfed4be-9797-4418-87b1-892b729b1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(prediction_output, train_output):\n",
    "        return - np.sum(train_output * np.log(prediction_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9e8123c0-e66a-4786-a50d-48a8fa6ae53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(weights, biases, outputs_per_layer, labels, learning_rate):\n",
    "        error = outputs_per_layer[-1] - labels\n",
    "    \n",
    "        REGULARIZAION_RATE = 0.01\n",
    "        for i in reversed(range(len(weights))):\n",
    "            y = outputs_per_layer[i]\n",
    "            y_1 = outputs_per_layer[i+1]\n",
    "\n",
    "            # back_error = (activation_function_prime(y_1) if i < len(weights) - 1 else 1) * error @ weights[i].T\n",
    "\n",
    "            # weights[i] -= learning_rate * (y.T @ error + REGULARIZAION_RATE * weights[i])\n",
    "            # biases[i] -= learning_rate * error.sum(axis = 0)\n",
    "\n",
    "            # error = back_error\n",
    "\n",
    "            delta = error * (activation_function_prime(y_1) if i < len(weights) - 1 else 1)\n",
    "\n",
    "            weights[i] -= learning_rate * (y.T @ delta + REGULARIZAION_RATE * weights[i]) / y.shape[0]\n",
    "            biases[i] -= learning_rate * np.sum(delta, axis=0, keepdims=True) / y.shape[0]\n",
    "    \n",
    "            error = delta @ weights[i].T\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad1db154-753f-42f6-93b8-31b1a6bb3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(y):\n",
    "    value_count = y.shape[1]\n",
    "    y = np.argmax(y, axis=1)\n",
    "    y = np.eye(value_count)[y]\n",
    "    return y\n",
    "\n",
    "def test_neural_network(test_input, weights, biases, test_output):\n",
    "    fp = forward_propagation_test(test_input, weights, biases)\n",
    "    predictions = predict(fp[-1])\n",
    "\n",
    "    accuracy = np.mean(np.sum(predictions * test_output, axis=1))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "264b051e-a907-41a5-9b82-b20ddf8151b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_learning_rate(current_learning_rate, accuracies, epoch, last_update_epoch):\n",
    "    min_learning_rate = 1e-6\n",
    "    patience = 7\n",
    "    factor = 0.5\n",
    "    \n",
    "    if epoch - last_update_epoch >= patience:\n",
    "        if len(accuracies) > patience and all(accuracies[-patience - 1] >= acc for acc in accuracies[-patience:]):\n",
    "            new_learning_rate = max(current_learning_rate * factor, min_learning_rate)\n",
    "            print(f\"Reducing learning rate from {current_learning_rate} to {new_learning_rate}\")\n",
    "            return new_learning_rate, epoch\n",
    "        \n",
    "    return current_learning_rate, last_update_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5d3ddb7d-52da-4293-96a3-21c110727ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(train_input, train_output, test_input, test_output):\n",
    "    \n",
    "    EPOCH_COUNT = 500\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    train_count = train_input.shape[0]\n",
    "    neurons_per_layer = [train_input.shape[1], 100, train_output.shape[1]]\n",
    "    weights, biases = initialize_weights_and_biases(neurons_per_layer)\n",
    "\n",
    "    accuracies_train = []\n",
    "    accuracies_test = []\n",
    "    learning_rate = 0.02\n",
    "    last_update_epoch = -1\n",
    "\n",
    "    for i in range(EPOCH_COUNT):\n",
    "        indices = np.arange(0, train_count)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_X = train_input[indices]\n",
    "        train_Y = train_output[indices]\n",
    "\n",
    "        for j in range(0, train_count, BATCH_SIZE):\n",
    "            batch_X = train_X[j:j + BATCH_SIZE]\n",
    "            batch_Y = train_Y[j:j + BATCH_SIZE]\n",
    "\n",
    "            outputs_per_layer = forward_propagation_train(batch_X, weights, biases)\n",
    "\n",
    "            back_propagation(weights, biases, outputs_per_layer, batch_Y, learning_rate)\n",
    "\n",
    "        acc_train = test_neural_network(train_input, weights, biases, train_output)\n",
    "        acc_test = test_neural_network(test_input, weights, biases, test_output)\n",
    "        accuracies_train.append(acc_train)\n",
    "        accuracies_test.append(acc_test)\n",
    "\n",
    "        learning_rate, last_update_epoch = update_learning_rate(learning_rate, accuracies_train, i, last_update_epoch)\n",
    "\n",
    "        print(f\"{i+1}: test:{acc_test * 100: .2f}% train:{acc_train * 100: .2f}% {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a952722d-9a9d-4071-b33c-bc54e4ee68b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: test: 88.55% train: 87.92% 0.02\n",
      "2: test: 90.75% train: 90.19% 0.02\n",
      "3: test: 91.77% train: 91.27% 0.02\n",
      "4: test: 92.34% train: 92.05% 0.02\n",
      "5: test: 92.87% train: 92.61% 0.02\n",
      "6: test: 93.31% train: 93.16% 0.02\n",
      "7: test: 93.63% train: 93.53% 0.02\n",
      "8: test: 93.89% train: 93.88% 0.02\n",
      "9: test: 94.28% train: 94.19% 0.02\n",
      "10: test: 94.48% train: 94.45% 0.02\n",
      "11: test: 94.74% train: 94.72% 0.02\n",
      "12: test: 94.87% train: 94.90% 0.02\n",
      "13: test: 95.03% train: 95.12% 0.02\n",
      "14: test: 95.27% train: 95.32% 0.02\n",
      "15: test: 95.29% train: 95.48% 0.02\n",
      "16: test: 95.40% train: 95.62% 0.02\n",
      "17: test: 95.53% train: 95.76% 0.02\n",
      "18: test: 95.76% train: 95.87% 0.02\n",
      "19: test: 95.83% train: 96.00% 0.02\n",
      "20: test: 95.93% train: 96.11% 0.02\n",
      "21: test: 96.00% train: 96.22% 0.02\n",
      "22: test: 96.18% train: 96.34% 0.02\n",
      "23: test: 96.16% train: 96.42% 0.02\n",
      "24: test: 96.24% train: 96.52% 0.02\n",
      "25: test: 96.34% train: 96.58% 0.02\n",
      "26: test: 96.42% train: 96.67% 0.02\n",
      "27: test: 96.57% train: 96.78% 0.02\n",
      "28: test: 96.61% train: 96.86% 0.02\n",
      "29: test: 96.63% train: 96.94% 0.02\n",
      "30: test: 96.76% train: 97.00% 0.02\n",
      "31: test: 96.70% train: 97.03% 0.02\n",
      "32: test: 96.75% train: 97.11% 0.02\n",
      "33: test: 96.70% train: 97.15% 0.02\n",
      "34: test: 96.85% train: 97.22% 0.02\n",
      "35: test: 96.88% train: 97.27% 0.02\n",
      "36: test: 96.88% train: 97.31% 0.02\n",
      "37: test: 97.01% train: 97.38% 0.02\n",
      "38: test: 96.96% train: 97.44% 0.02\n",
      "39: test: 96.88% train: 97.47% 0.02\n",
      "40: test: 97.01% train: 97.53% 0.02\n",
      "41: test: 97.02% train: 97.58% 0.02\n",
      "42: test: 96.95% train: 97.59% 0.02\n",
      "43: test: 97.06% train: 97.65% 0.02\n",
      "44: test: 97.10% train: 97.67% 0.02\n",
      "45: test: 97.08% train: 97.71% 0.02\n",
      "46: test: 97.13% train: 97.76% 0.02\n",
      "47: test: 97.19% train: 97.77% 0.02\n",
      "48: test: 97.22% train: 97.79% 0.02\n",
      "49: test: 97.22% train: 97.86% 0.02\n",
      "50: test: 97.29% train: 97.84% 0.02\n",
      "51: test: 97.31% train: 97.90% 0.02\n",
      "52: test: 97.31% train: 97.94% 0.02\n",
      "53: test: 97.39% train: 97.95% 0.02\n",
      "54: test: 97.35% train: 97.98% 0.02\n",
      "55: test: 97.35% train: 98.01% 0.02\n",
      "56: test: 97.36% train: 98.04% 0.02\n",
      "57: test: 97.41% train: 98.06% 0.02\n",
      "58: test: 97.41% train: 98.09% 0.02\n",
      "59: test: 97.40% train: 98.11% 0.02\n",
      "60: test: 97.44% train: 98.14% 0.02\n",
      "61: test: 97.44% train: 98.17% 0.02\n",
      "62: test: 97.52% train: 98.19% 0.02\n",
      "63: test: 97.44% train: 98.20% 0.02\n",
      "64: test: 97.47% train: 98.21% 0.02\n",
      "65: test: 97.46% train: 98.26% 0.02\n",
      "66: test: 97.52% train: 98.27% 0.02\n",
      "67: test: 97.53% train: 98.30% 0.02\n",
      "68: test: 97.58% train: 98.32% 0.02\n",
      "69: test: 97.56% train: 98.34% 0.02\n",
      "70: test: 97.56% train: 98.33% 0.02\n",
      "71: test: 97.63% train: 98.35% 0.02\n",
      "72: test: 97.55% train: 98.40% 0.02\n",
      "73: test: 97.58% train: 98.41% 0.02\n",
      "74: test: 97.61% train: 98.40% 0.02\n",
      "75: test: 97.65% train: 98.44% 0.02\n",
      "76: test: 97.67% train: 98.46% 0.02\n",
      "77: test: 97.70% train: 98.48% 0.02\n",
      "78: test: 97.69% train: 98.49% 0.02\n",
      "79: test: 97.69% train: 98.51% 0.02\n",
      "80: test: 97.70% train: 98.52% 0.02\n",
      "81: test: 97.75% train: 98.53% 0.02\n",
      "82: test: 97.77% train: 98.55% 0.02\n",
      "83: test: 97.70% train: 98.56% 0.02\n",
      "84: test: 97.72% train: 98.57% 0.02\n",
      "85: test: 97.71% train: 98.58% 0.02\n",
      "86: test: 97.80% train: 98.61% 0.02\n",
      "87: test: 97.81% train: 98.60% 0.02\n",
      "88: test: 97.79% train: 98.63% 0.02\n",
      "89: test: 97.83% train: 98.63% 0.02\n",
      "90: test: 97.85% train: 98.64% 0.02\n",
      "91: test: 97.81% train: 98.66% 0.02\n",
      "92: test: 97.83% train: 98.69% 0.02\n",
      "93: test: 97.88% train: 98.68% 0.02\n",
      "94: test: 97.85% train: 98.70% 0.02\n",
      "95: test: 97.93% train: 98.72% 0.02\n",
      "96: test: 97.83% train: 98.72% 0.02\n",
      "97: test: 97.91% train: 98.73% 0.02\n",
      "98: test: 97.92% train: 98.77% 0.02\n",
      "99: test: 97.89% train: 98.76% 0.02\n",
      "100: test: 97.89% train: 98.78% 0.02\n",
      "101: test: 97.89% train: 98.79% 0.02\n",
      "102: test: 97.96% train: 98.79% 0.02\n",
      "103: test: 97.95% train: 98.80% 0.02\n",
      "104: test: 97.95% train: 98.83% 0.02\n",
      "105: test: 97.91% train: 98.83% 0.02\n",
      "106: test: 97.94% train: 98.84% 0.02\n",
      "107: test: 97.95% train: 98.85% 0.02\n",
      "108: test: 97.99% train: 98.86% 0.02\n",
      "109: test: 97.93% train: 98.89% 0.02\n",
      "110: test: 97.97% train: 98.90% 0.02\n",
      "111: test: 97.97% train: 98.91% 0.02\n",
      "112: test: 97.94% train: 98.91% 0.02\n",
      "113: test: 97.94% train: 98.93% 0.02\n",
      "114: test: 97.94% train: 98.93% 0.02\n",
      "115: test: 97.96% train: 98.94% 0.02\n",
      "116: test: 98.01% train: 98.95% 0.02\n",
      "117: test: 98.02% train: 98.97% 0.02\n",
      "118: test: 97.93% train: 98.99% 0.02\n",
      "119: test: 97.99% train: 98.98% 0.02\n",
      "120: test: 97.99% train: 99.01% 0.02\n",
      "121: test: 97.94% train: 99.00% 0.02\n",
      "122: test: 98.00% train: 99.00% 0.02\n",
      "123: test: 97.99% train: 99.02% 0.02\n",
      "124: test: 97.96% train: 99.05% 0.02\n",
      "125: test: 98.00% train: 99.05% 0.02\n",
      "126: test: 97.98% train: 99.04% 0.02\n",
      "127: test: 98.02% train: 99.06% 0.02\n",
      "128: test: 97.99% train: 99.04% 0.02\n",
      "129: test: 98.00% train: 99.07% 0.02\n",
      "130: test: 97.97% train: 99.09% 0.02\n",
      "131: test: 98.03% train: 99.08% 0.02\n",
      "132: test: 98.04% train: 99.11% 0.02\n",
      "133: test: 98.05% train: 99.09% 0.02\n",
      "134: test: 98.05% train: 99.13% 0.02\n",
      "135: test: 97.97% train: 99.11% 0.02\n",
      "136: test: 98.01% train: 99.11% 0.02\n",
      "137: test: 98.03% train: 99.13% 0.02\n",
      "138: test: 97.99% train: 99.14% 0.02\n",
      "139: test: 98.02% train: 99.13% 0.02\n",
      "140: test: 98.02% train: 99.15% 0.02\n",
      "141: test: 98.06% train: 99.17% 0.02\n",
      "142: test: 98.03% train: 99.16% 0.02\n",
      "143: test: 98.03% train: 99.16% 0.02\n",
      "144: test: 98.02% train: 99.17% 0.02\n",
      "145: test: 98.01% train: 99.18% 0.02\n",
      "146: test: 98.06% train: 99.18% 0.02\n",
      "147: test: 97.99% train: 99.18% 0.02\n",
      "148: test: 98.03% train: 99.21% 0.02\n",
      "149: test: 98.03% train: 99.21% 0.02\n",
      "150: test: 98.08% train: 99.22% 0.02\n",
      "151: test: 98.04% train: 99.22% 0.02\n",
      "152: test: 98.04% train: 99.23% 0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train_X, train_Y, test_X, test_Y \u001b[38;5;241m=\u001b[39m transform_initial_data(i_train_X, i_train_Y, i_test_X, i_test_Y)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_Y\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[124], line 30\u001b[0m, in \u001b[0;36mtrain_neural_network\u001b[1;34m(train_input, train_output, test_input, test_output)\u001b[0m\n\u001b[0;32m     26\u001b[0m     outputs_per_layer \u001b[38;5;241m=\u001b[39m forward_propagation_train(batch_X, weights, biases)\n\u001b[0;32m     28\u001b[0m     back_propagation(weights, biases, outputs_per_layer, batch_Y, learning_rate)\n\u001b[1;32m---> 30\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m \u001b[43mtest_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m acc_test \u001b[38;5;241m=\u001b[39m test_neural_network(test_input, weights, biases, test_output)\n\u001b[0;32m     32\u001b[0m accuracies_train\u001b[38;5;241m.\u001b[39mappend(acc_train)\n",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m, in \u001b[0;36mtest_neural_network\u001b[1;34m(test_input, weights, biases, test_output)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_neural_network\u001b[39m(test_input, weights, biases, test_output):\n\u001b[1;32m----> 8\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict(fp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     11\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39msum(predictions \u001b[38;5;241m*\u001b[39m test_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[112], line 32\u001b[0m, in \u001b[0;36mforward_propagation_test\u001b[1;34m(inputs, weights, biases)\u001b[0m\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m output_per_layer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m     z \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m w \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m---> 32\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m softmax(z)\n\u001b[0;32m     33\u001b[0m     output_per_layer\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_per_layer\n",
      "Cell \u001b[1;32mIn[126], line 15\u001b[0m, in \u001b[0;36mleaky_relu\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m*\u001b[39my\n\u001b[0;32m     13\u001b[0m RELU_LEAK \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mleaky_relu\u001b[39m(z):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(z\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m, z, z\u001b[38;5;241m*\u001b[39mRELU_LEAK)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mleaky_relu_prime\u001b[39m(y):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_X, train_Y, test_X, test_Y = transform_initial_data(i_train_X, i_train_Y, i_test_X, i_test_Y)\n",
    "\n",
    "train_neural_network(train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680b606-2e7b-4e3d-80fc-48b51a961254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62e663-0cbd-4d0f-a304-d5c87b578104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
