{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902f5dea-0226-40ea-bd2e-1798e3ea0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data1',\n",
    "        transform=lambda x: np.array(x).flatten(),\n",
    "        download=True,\n",
    "        train=is_train)\n",
    "    \n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    \n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "        \n",
    "    return mnist_data, mnist_labels\n",
    "    \n",
    "i_train_X, i_train_Y = download_mnist(True)\n",
    "i_test_X, i_test_Y = download_mnist(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f491c42f-0075-4c6a-9f44-d3f6d18286f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot_encoding(labels: np.ndarray) -> np.ndarray:\n",
    "    return np.eye(10)[labels]\n",
    "\n",
    "def normalized(input: np.ndarray) -> np.ndarray:\n",
    "    return input / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaa8a9a1-e22e-416a-b358-a79feded229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_initial_data(train_X, train_Y, test_X, test_Y):\n",
    "    train_Y = convert_to_one_hot_encoding(train_Y)\n",
    "    test_Y = convert_to_one_hot_encoding(test_Y)\n",
    "    \n",
    "    train_X = np.array(train_X)\n",
    "    test_X = np.array(test_X)\n",
    "\n",
    "    train_X = normalized(train_X)\n",
    "    test_X = normalized(test_X)\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "073dd600-6f69-4743-9489-96dc74f063bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def xavier_uniform(fan_in, fan_out):\n",
    "    return math.sqrt(6/(fan_in + fan_out))\n",
    "\n",
    "def he_uniform(fan_in, fan_out):\n",
    "    return math.sqrt(6 / fan_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17074afd-177c-4764-8c2e-db5c023f6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_and_biases(neurons_per_layer):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(1, len(neurons_per_layer)):\n",
    "        fan_in, fan_out = neurons_per_layer[i - 1], neurons_per_layer[i]\n",
    "        limit = he_uniform(fan_in, fan_out) #if i < len(neurons_per_layer) - 1 else he_uniform(fan_in, fan_out)\n",
    "        \n",
    "        weights.append(np.random.uniform(low=-limit, high=limit, size=(fan_in, fan_out)))\n",
    "        biases.append(np.zeros((1, fan_out)))\n",
    "        \n",
    "    return weights, biases\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a980fcae-6c5a-4621-b879-54e81856273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z)) #-np.clip(z, -100, 100)\n",
    "\n",
    "def sigmoid_prime(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def tanh(z):\n",
    "    return 2*sigmoid(2*z) - 1\n",
    "\n",
    "def tanh_prime(y):\n",
    "    return 1 - y*y\n",
    "\n",
    "RELU_LEAK = 0.01\n",
    "\n",
    "def leaky_relu(z):\n",
    "    return np.where(z>0, z, z*RELU_LEAK)\n",
    "\n",
    "def leaky_relu_prime(y):\n",
    "    return np.where(y>0, 1, RELU_LEAK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea199ffe-8386-43c4-aca0-568688e50758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return e_z / e_z.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316bdd7b-0ef0-43ce-a100-d091449586ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "        'leaky_relu': (leaky_relu, leaky_relu_prime),\n",
    "        'sigmoid': (sigmoid, sigmoid_prime),\n",
    "        'tanh': (tanh, tanh_prime)\n",
    "    }\n",
    "\n",
    "activation_function, activation_function_prime = activation_functions['leaky_relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826aff6e-52cc-435b-a7a0-98b935e565bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_train(inputs, weights, biases):\n",
    "    output_per_layer = [inputs]\n",
    "    DROPOUT_RATE = 0.1\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        w = weights[i]\n",
    "        b = biases[i]\n",
    "        x = output_per_layer[-1]\n",
    "        \n",
    "        z = x @ w + b\n",
    "\n",
    "        #DROPOUT\n",
    "        if i < len(weights) - 1:\n",
    "            #z *= np.random.binomial(1, 1 - DROPOUT_RATE, z.shape)\n",
    "            z *= np.random.choice([0, 1/(1-DROPOUT_RATE)], z.shape, p=[DROPOUT_RATE, 1-DROPOUT_RATE]) #[0, 1/(1-DROPOUT_RATE)]\n",
    "            \n",
    "        y = activation_function(z) if i < len(weights) - 1 else softmax(z)\n",
    "        output_per_layer.append(y)\n",
    "        \n",
    "    return output_per_layer\n",
    "\n",
    "def forward_propagation_test(inputs, weights, biases):\n",
    "    output_per_layer = [inputs]\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        w = weights[i]\n",
    "        b = biases[i]\n",
    "        x = output_per_layer[-1]\n",
    "        \n",
    "        z = x @ w + b\n",
    "        \n",
    "        y = activation_function(z) if i < len(weights) - 1 else softmax(z)\n",
    "        output_per_layer.append(y)\n",
    "        \n",
    "    return output_per_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dfed4be-9797-4418-87b1-892b729b1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(prediction_output, train_output):\n",
    "        return - np.sum(train_output * np.log(prediction_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e8123c0-e66a-4786-a50d-48a8fa6ae53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(weights, biases, outputs_per_layer, labels, learning_rate):\n",
    "        error = outputs_per_layer[-1] - labels\n",
    "    \n",
    "        REGULARIZAION_RATE = 0.01\n",
    "        for i in reversed(range(len(weights))):\n",
    "            y = outputs_per_layer[i]\n",
    "            y_1 = outputs_per_layer[i+1]\n",
    "\n",
    "            delta = error * (activation_function_prime(y_1) if i < len(weights) - 1 else 1)\n",
    "            back_error = delta @ weights[i].T\n",
    "\n",
    "            weights[i] -= learning_rate * (y.T @ delta + REGULARIZAION_RATE * weights[i]) / y.shape[0]\n",
    "            biases[i] -= learning_rate * np.sum(delta, axis=0, keepdims=True) / y.shape[0]\n",
    "    \n",
    "            error = back_error\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad1db154-753f-42f6-93b8-31b1a6bb3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(y):\n",
    "    value_count = y.shape[1]\n",
    "    y = np.argmax(y, axis=1)\n",
    "    y = np.eye(value_count)[y]\n",
    "    return y\n",
    "\n",
    "def test_neural_network(test_input, weights, biases, test_output):\n",
    "    fp = forward_propagation_test(test_input, weights, biases)\n",
    "    predictions = predict(fp[-1])\n",
    "\n",
    "    accuracy = np.mean(np.sum(predictions * test_output, axis=1))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "264b051e-a907-41a5-9b82-b20ddf8151b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_learning_rate(current_learning_rate, accuracies, epoch, last_update_epoch):\n",
    "    min_learning_rate = 1e-6\n",
    "    patience = 7\n",
    "    factor = 0.5\n",
    "    \n",
    "    if epoch - last_update_epoch >= patience:\n",
    "        if len(accuracies) > patience and all(accuracies[-patience - 1] >= acc for acc in accuracies[-patience:]):\n",
    "            new_learning_rate = max(current_learning_rate * factor, min_learning_rate)\n",
    "            print(f\"Reducing learning rate from {current_learning_rate} to {new_learning_rate}\")\n",
    "            return new_learning_rate, epoch\n",
    "        \n",
    "    return current_learning_rate, last_update_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d3ddb7d-52da-4293-96a3-21c110727ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(train_input, train_output, test_input, test_output):\n",
    "    \n",
    "    EPOCH_COUNT = 500\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    train_count = train_input.shape[0]\n",
    "    neurons_per_layer = [train_input.shape[1], 100, train_output.shape[1]]\n",
    "    weights, biases = initialize_weights_and_biases(neurons_per_layer)\n",
    "\n",
    "    accuracies_train = []\n",
    "    accuracies_test = []\n",
    "    learning_rate = 0.02\n",
    "    last_update_epoch = -1\n",
    "\n",
    "    for i in range(EPOCH_COUNT):\n",
    "        indices = np.arange(0, train_count)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_X = train_input[indices]\n",
    "        train_Y = train_output[indices]\n",
    "\n",
    "        for j in range(0, train_count, BATCH_SIZE):\n",
    "            batch_X = train_X[j:j + BATCH_SIZE]\n",
    "            batch_Y = train_Y[j:j + BATCH_SIZE]\n",
    "\n",
    "            outputs_per_layer = forward_propagation_train(batch_X, weights, biases)\n",
    "\n",
    "            back_propagation(weights, biases, outputs_per_layer, batch_Y, learning_rate)\n",
    "\n",
    "        acc_train = test_neural_network(train_input, weights, biases, train_output)\n",
    "        acc_test = test_neural_network(test_input, weights, biases, test_output)\n",
    "        accuracies_train.append(acc_train)\n",
    "        accuracies_test.append(acc_test)\n",
    "\n",
    "        learning_rate, last_update_epoch = update_learning_rate(learning_rate, accuracies_train, i, last_update_epoch)\n",
    "\n",
    "        print(f\"epoch {i+1}: test:{acc_test * 100: .2f}% ; train:{acc_train * 100: .2f}% ; learning_rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a952722d-9a9d-4071-b33c-bc54e4ee68b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: test: 89.06% ; train: 88.04% ; learning_rate: 0.02\n",
      "epoch 2: test: 91.00% ; train: 90.17% ; learning_rate: 0.02\n",
      "epoch 3: test: 91.83% ; train: 91.22% ; learning_rate: 0.02\n",
      "epoch 4: test: 92.44% ; train: 91.93% ; learning_rate: 0.02\n",
      "epoch 5: test: 92.93% ; train: 92.53% ; learning_rate: 0.02\n",
      "epoch 6: test: 93.28% ; train: 93.09% ; learning_rate: 0.02\n",
      "epoch 7: test: 93.56% ; train: 93.44% ; learning_rate: 0.02\n",
      "epoch 8: test: 93.81% ; train: 93.69% ; learning_rate: 0.02\n",
      "epoch 9: test: 94.20% ; train: 94.07% ; learning_rate: 0.02\n",
      "epoch 10: test: 94.48% ; train: 94.36% ; learning_rate: 0.02\n",
      "epoch 11: test: 94.52% ; train: 94.57% ; learning_rate: 0.02\n",
      "epoch 12: test: 94.85% ; train: 94.80% ; learning_rate: 0.02\n",
      "epoch 13: test: 94.91% ; train: 94.98% ; learning_rate: 0.02\n",
      "epoch 14: test: 95.13% ; train: 95.18% ; learning_rate: 0.02\n",
      "epoch 15: test: 95.24% ; train: 95.31% ; learning_rate: 0.02\n",
      "epoch 16: test: 95.36% ; train: 95.46% ; learning_rate: 0.02\n",
      "epoch 17: test: 95.43% ; train: 95.60% ; learning_rate: 0.02\n",
      "epoch 18: test: 95.52% ; train: 95.78% ; learning_rate: 0.02\n",
      "epoch 19: test: 95.64% ; train: 95.91% ; learning_rate: 0.02\n",
      "epoch 20: test: 95.70% ; train: 96.06% ; learning_rate: 0.02\n",
      "epoch 21: test: 95.69% ; train: 96.13% ; learning_rate: 0.02\n",
      "epoch 22: test: 95.82% ; train: 96.25% ; learning_rate: 0.02\n",
      "epoch 23: test: 95.85% ; train: 96.35% ; learning_rate: 0.02\n",
      "epoch 24: test: 95.86% ; train: 96.43% ; learning_rate: 0.02\n",
      "epoch 25: test: 95.98% ; train: 96.54% ; learning_rate: 0.02\n",
      "epoch 26: test: 96.07% ; train: 96.62% ; learning_rate: 0.02\n",
      "epoch 27: test: 96.19% ; train: 96.72% ; learning_rate: 0.02\n",
      "epoch 28: test: 96.26% ; train: 96.78% ; learning_rate: 0.02\n",
      "epoch 29: test: 96.30% ; train: 96.82% ; learning_rate: 0.02\n",
      "epoch 30: test: 96.36% ; train: 96.92% ; learning_rate: 0.02\n",
      "epoch 31: test: 96.34% ; train: 96.98% ; learning_rate: 0.02\n",
      "epoch 32: test: 96.36% ; train: 97.05% ; learning_rate: 0.02\n",
      "epoch 33: test: 96.48% ; train: 97.07% ; learning_rate: 0.02\n",
      "epoch 34: test: 96.59% ; train: 97.16% ; learning_rate: 0.02\n",
      "epoch 35: test: 96.60% ; train: 97.17% ; learning_rate: 0.02\n",
      "epoch 36: test: 96.65% ; train: 97.24% ; learning_rate: 0.02\n",
      "epoch 37: test: 96.74% ; train: 97.32% ; learning_rate: 0.02\n",
      "epoch 38: test: 96.79% ; train: 97.38% ; learning_rate: 0.02\n",
      "epoch 39: test: 96.86% ; train: 97.41% ; learning_rate: 0.02\n",
      "epoch 40: test: 96.87% ; train: 97.46% ; learning_rate: 0.02\n",
      "epoch 41: test: 96.91% ; train: 97.49% ; learning_rate: 0.02\n",
      "epoch 42: test: 96.87% ; train: 97.53% ; learning_rate: 0.02\n",
      "epoch 43: test: 96.94% ; train: 97.56% ; learning_rate: 0.02\n",
      "epoch 44: test: 97.02% ; train: 97.61% ; learning_rate: 0.02\n",
      "epoch 45: test: 97.03% ; train: 97.64% ; learning_rate: 0.02\n",
      "epoch 46: test: 97.06% ; train: 97.67% ; learning_rate: 0.02\n",
      "epoch 47: test: 97.07% ; train: 97.71% ; learning_rate: 0.02\n",
      "epoch 48: test: 97.10% ; train: 97.74% ; learning_rate: 0.02\n",
      "epoch 49: test: 97.16% ; train: 97.75% ; learning_rate: 0.02\n",
      "epoch 50: test: 97.13% ; train: 97.81% ; learning_rate: 0.02\n",
      "epoch 51: test: 97.14% ; train: 97.84% ; learning_rate: 0.02\n",
      "epoch 52: test: 97.17% ; train: 97.88% ; learning_rate: 0.02\n",
      "epoch 53: test: 97.23% ; train: 97.89% ; learning_rate: 0.02\n",
      "epoch 54: test: 97.28% ; train: 97.93% ; learning_rate: 0.02\n",
      "epoch 55: test: 97.27% ; train: 97.95% ; learning_rate: 0.02\n",
      "epoch 56: test: 97.32% ; train: 97.96% ; learning_rate: 0.02\n",
      "epoch 57: test: 97.33% ; train: 98.00% ; learning_rate: 0.02\n",
      "epoch 58: test: 97.30% ; train: 98.04% ; learning_rate: 0.02\n",
      "epoch 59: test: 97.35% ; train: 98.04% ; learning_rate: 0.02\n",
      "epoch 60: test: 97.35% ; train: 98.06% ; learning_rate: 0.02\n",
      "epoch 61: test: 97.38% ; train: 98.11% ; learning_rate: 0.02\n",
      "epoch 62: test: 97.40% ; train: 98.13% ; learning_rate: 0.02\n",
      "epoch 63: test: 97.37% ; train: 98.15% ; learning_rate: 0.02\n",
      "epoch 64: test: 97.35% ; train: 98.18% ; learning_rate: 0.02\n",
      "epoch 65: test: 97.37% ; train: 98.18% ; learning_rate: 0.02\n",
      "epoch 66: test: 97.44% ; train: 98.20% ; learning_rate: 0.02\n",
      "epoch 67: test: 97.44% ; train: 98.22% ; learning_rate: 0.02\n",
      "epoch 68: test: 97.44% ; train: 98.25% ; learning_rate: 0.02\n",
      "epoch 69: test: 97.43% ; train: 98.28% ; learning_rate: 0.02\n",
      "epoch 70: test: 97.44% ; train: 98.28% ; learning_rate: 0.02\n",
      "epoch 71: test: 97.47% ; train: 98.28% ; learning_rate: 0.02\n",
      "epoch 72: test: 97.49% ; train: 98.32% ; learning_rate: 0.02\n",
      "epoch 73: test: 97.44% ; train: 98.33% ; learning_rate: 0.02\n",
      "epoch 74: test: 97.53% ; train: 98.36% ; learning_rate: 0.02\n",
      "epoch 75: test: 97.56% ; train: 98.37% ; learning_rate: 0.02\n",
      "epoch 76: test: 97.54% ; train: 98.40% ; learning_rate: 0.02\n",
      "epoch 77: test: 97.52% ; train: 98.40% ; learning_rate: 0.02\n",
      "epoch 78: test: 97.59% ; train: 98.41% ; learning_rate: 0.02\n",
      "epoch 79: test: 97.56% ; train: 98.43% ; learning_rate: 0.02\n",
      "epoch 80: test: 97.63% ; train: 98.46% ; learning_rate: 0.02\n",
      "epoch 81: test: 97.57% ; train: 98.48% ; learning_rate: 0.02\n",
      "epoch 82: test: 97.61% ; train: 98.47% ; learning_rate: 0.02\n",
      "epoch 83: test: 97.59% ; train: 98.51% ; learning_rate: 0.02\n",
      "epoch 84: test: 97.60% ; train: 98.51% ; learning_rate: 0.02\n",
      "epoch 85: test: 97.64% ; train: 98.52% ; learning_rate: 0.02\n",
      "epoch 86: test: 97.69% ; train: 98.55% ; learning_rate: 0.02\n",
      "epoch 87: test: 97.63% ; train: 98.57% ; learning_rate: 0.02\n",
      "epoch 88: test: 97.68% ; train: 98.58% ; learning_rate: 0.02\n",
      "epoch 89: test: 97.63% ; train: 98.59% ; learning_rate: 0.02\n",
      "epoch 90: test: 97.63% ; train: 98.61% ; learning_rate: 0.02\n",
      "epoch 91: test: 97.66% ; train: 98.62% ; learning_rate: 0.02\n",
      "epoch 92: test: 97.69% ; train: 98.63% ; learning_rate: 0.02\n",
      "epoch 93: test: 97.66% ; train: 98.66% ; learning_rate: 0.02\n",
      "epoch 94: test: 97.67% ; train: 98.65% ; learning_rate: 0.02\n",
      "epoch 95: test: 97.66% ; train: 98.66% ; learning_rate: 0.02\n",
      "epoch 96: test: 97.64% ; train: 98.67% ; learning_rate: 0.02\n",
      "epoch 97: test: 97.69% ; train: 98.69% ; learning_rate: 0.02\n",
      "epoch 98: test: 97.70% ; train: 98.71% ; learning_rate: 0.02\n",
      "epoch 99: test: 97.70% ; train: 98.73% ; learning_rate: 0.02\n",
      "epoch 100: test: 97.68% ; train: 98.75% ; learning_rate: 0.02\n",
      "epoch 101: test: 97.67% ; train: 98.74% ; learning_rate: 0.02\n",
      "epoch 102: test: 97.70% ; train: 98.75% ; learning_rate: 0.02\n",
      "epoch 103: test: 97.71% ; train: 98.75% ; learning_rate: 0.02\n",
      "epoch 104: test: 97.76% ; train: 98.78% ; learning_rate: 0.02\n",
      "epoch 105: test: 97.73% ; train: 98.80% ; learning_rate: 0.02\n",
      "epoch 106: test: 97.76% ; train: 98.80% ; learning_rate: 0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train_X, train_Y, test_X, test_Y \u001b[38;5;241m=\u001b[39m transform_initial_data(i_train_X, i_train_Y, i_test_X, i_test_Y)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_Y\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 28\u001b[0m, in \u001b[0;36mtrain_neural_network\u001b[1;34m(train_input, train_output, test_input, test_output)\u001b[0m\n\u001b[0;32m     24\u001b[0m     batch_Y \u001b[38;5;241m=\u001b[39m train_Y[j:j \u001b[38;5;241m+\u001b[39m BATCH_SIZE]\n\u001b[0;32m     26\u001b[0m     outputs_per_layer \u001b[38;5;241m=\u001b[39m forward_propagation_train(batch_X, weights, biases)\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mback_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_per_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m test_neural_network(train_input, weights, biases, train_output)\n\u001b[0;32m     31\u001b[0m acc_test \u001b[38;5;241m=\u001b[39m test_neural_network(test_input, weights, biases, test_output)\n",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m, in \u001b[0;36mback_propagation\u001b[1;34m(weights, biases, outputs_per_layer, labels, learning_rate)\u001b[0m\n\u001b[0;32m     10\u001b[0m back_error \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m@\u001b[39m weights[i]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     12\u001b[0m weights[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m (y\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m delta \u001b[38;5;241m+\u001b[39m REGULARIZAION_RATE \u001b[38;5;241m*\u001b[39m weights[i]) \u001b[38;5;241m/\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m biases[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m error \u001b[38;5;241m=\u001b[39m back_error\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2344\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2338\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `min` or `max` keyword argument when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2339\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2345\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[0;32m   2349\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[0;32m   2350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2351\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_X, train_Y, test_X, test_Y = transform_initial_data(i_train_X, i_train_Y, i_test_X, i_test_Y)\n",
    "\n",
    "train_neural_network(train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680b606-2e7b-4e3d-80fc-48b51a961254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62e663-0cbd-4d0f-a304-d5c87b578104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
