{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902f5dea-0226-40ea-bd2e-1798e3ea0a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data1\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data1\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data1\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data1\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data1\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data1\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data1\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data1\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data1\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data1\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data1\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data1\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data1',\n",
    "        transform=lambda x: np.array(x).flatten(),\n",
    "        download=True,\n",
    "        train=is_train)\n",
    "    \n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    \n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "        \n",
    "    return mnist_data, mnist_labels\n",
    "    \n",
    "i_train_X, i_train_Y = download_mnist(True)\n",
    "i_test_X, i_test_Y = download_mnist(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f491c42f-0075-4c6a-9f44-d3f6d18286f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot_encoding(labels: np.ndarray) -> np.ndarray:\n",
    "    return np.eye(10)[labels]\n",
    "\n",
    "def normalized(input: np.ndarray) -> np.ndarray:\n",
    "    return input / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa8a9a1-e22e-416a-b358-a79feded229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_initial_data(train_X, train_Y, test_X, test_Y):\n",
    "    train_Y = convert_to_one_hot_encoding(train_Y)\n",
    "    test_Y = convert_to_one_hot_encoding(test_Y)\n",
    "    \n",
    "    train_X = np.array(train_X)\n",
    "    test_X = np.array(test_X)\n",
    "\n",
    "    train_X = normalized(train_X)\n",
    "    test_X = normalized(test_X)\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073dd600-6f69-4743-9489-96dc74f063bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def xavier_uniform(fan_in, fan_out):\n",
    "    return math.sqrt(6/(fan_in + fan_out))\n",
    "\n",
    "def he_uniform(fan_in, fan_out):\n",
    "    return math.sqrt(6 / fan_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17074afd-177c-4764-8c2e-db5c023f6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_and_biases(neurons_per_layer):\n",
    "    limits = [\n",
    "        he_uniform(neurons_per_layer[i-1], neurons_per_layer[i])\n",
    "        for i in range(1, len(neurons_per_layer))\n",
    "    ]\n",
    "\n",
    "    weights = [\n",
    "        np.random.uniform(low=-limits[i-1], high=limits[i-1], size=(neurons_per_layer[i-1], neurons_per_layer[i]))\n",
    "        for i in range(1, len(neurons_per_layer))\n",
    "    ]\n",
    "\n",
    "    biases = [\n",
    "        #np.random.uniform(low=-limits[i-1], high=limits[i-1], size=(1, neurons_per_layer[i]))\n",
    "        np.zeros((1, neurons_per_layer[i]))\n",
    "        for i in range(1, len(neurons_per_layer))\n",
    "    ]\n",
    "\n",
    "    return weights, biases\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a980fcae-6c5a-4621-b879-54e81856273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def tanh(z):\n",
    "    return 2*sigmoid(2*z) - 1\n",
    "\n",
    "def tanh_prime(y):\n",
    "    return 1 - y*y\n",
    "\n",
    "RELU_LEAK = 0.01\n",
    "\n",
    "def leaky_relu(z):\n",
    "    return np.where(z>0, z, z*RELU_LEAK)\n",
    "\n",
    "def leaky_relu_prime(y):\n",
    "    return np.where(y>0, 1, RELU_LEAK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea199ffe-8386-43c4-aca0-568688e50758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return e_z / e_z.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "316bdd7b-0ef0-43ce-a100-d091449586ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "        'leaky_relu': (leaky_relu, leaky_relu_prime),\n",
    "        'sigmoid': (sigmoid, sigmoid_prime),\n",
    "        'tanh': (tanh, tanh_prime)\n",
    "    }\n",
    "\n",
    "activation_function, activation_function_prime = activation_functions['leaky_relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "826aff6e-52cc-435b-a7a0-98b935e565bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_train(inputs, weights, biases):\n",
    "    output_per_layer = [inputs]\n",
    "    DROPOUT_RATE = 0.0\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        w = weights[i]\n",
    "        b = biases[i]\n",
    "        x = output_per_layer[-1]\n",
    "        \n",
    "        z = x @ w + b\n",
    "\n",
    "        #DROPOUT\n",
    "        if i < len(weights) - 1:\n",
    "            z *= np.random.choice([0, 1/(1-DROPOUT_RATE)], z.shape, p=[DROPOUT_RATE, 1-DROPOUT_RATE])\n",
    "            \n",
    "        y = activation_function(z) if i < len(weights) - 1 else softmax(z)\n",
    "        output_per_layer.append(y)\n",
    "        \n",
    "    return output_per_layer\n",
    "\n",
    "def forward_propagation_test(inputs, weights, biases):\n",
    "    output_per_layer = [inputs]\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        w = weights[i]\n",
    "        b = biases[i]\n",
    "        x = output_per_layer[-1]\n",
    "        \n",
    "        z = x @ w + b\n",
    "        \n",
    "        y = activation_function(z) if i < len(weights) - 1 else softmax(z)\n",
    "        output_per_layer.append(y)\n",
    "        \n",
    "    return output_per_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dfed4be-9797-4418-87b1-892b729b1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(prediction_output, train_output):\n",
    "        return - np.sum(train_output * np.log(prediction_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9e8123c0-e66a-4786-a50d-48a8fa6ae53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(weights, biases, outputs_per_layer, labels, learning_rate):\n",
    "        error = outputs_per_layer[-1] - labels\n",
    "        #LEARNING_RATE = 0.0001\n",
    "        FORGET_RATE = 0.00001\n",
    "        for i in reversed(range(len(weights))):\n",
    "            y = outputs_per_layer[i]\n",
    "            y_1 = outputs_per_layer[i+1]\n",
    "\n",
    "            back_error = activation_function_prime(y_1) * error @ weights[i].T\n",
    "\n",
    "            weights[i] -= learning_rate * (y.T @ error  + FORGET_RATE * weights[i])\n",
    "            biases[i] -= learning_rate * error.sum(axis = 0)\n",
    "\n",
    "            error = back_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad1db154-753f-42f6-93b8-31b1a6bb3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(y):\n",
    "    value_count = y.shape[1]\n",
    "    y = np.argmax(y, axis=1)\n",
    "    y = np.eye(value_count)[y]\n",
    "    return y\n",
    "\n",
    "def test_neural_network(test_input, weights, biases, test_output):\n",
    "    fp = forward_propagation_test(test_input, weights, biases)\n",
    "    predictions = predict(fp[-1])\n",
    "\n",
    "    accuracy = np.mean(np.sum(predictions * test_output, axis=1))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "264b051e-a907-41a5-9b82-b20ddf8151b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_learning_rate(current_learning_rate, accuracies, epoch, last_update_epoch):\n",
    "    min_learning_rate = 1e-6\n",
    "    patience = 7\n",
    "    factor = 0.5\n",
    "    \n",
    "    if epoch - last_update_epoch >= patience:\n",
    "        if len(accuracies) > patience and all(accuracies[-patience - 1] >= acc for acc in accuracies[-patience:]):\n",
    "            new_learning_rate = max(current_learning_rate * factor, min_learning_rate)\n",
    "            print(f\"Reducing learning rate from {current_learning_rate} to {new_learning_rate}\")\n",
    "            return new_learning_rate, epoch\n",
    "        \n",
    "    return current_learning_rate, last_update_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d3ddb7d-52da-4293-96a3-21c110727ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(train_input, train_output, test_input, test_output):\n",
    "    \n",
    "    EPOCH_COUNT = 500\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    train_count = train_input.shape[0]\n",
    "    neurons_per_layer = [train_input.shape[1], 100, train_output.shape[1]]\n",
    "    weights, biases = initialize_weights_and_biases(neurons_per_layer)\n",
    "\n",
    "    accuracies_train = []\n",
    "    accuracies_test = []\n",
    "    learning_rate = 0.001\n",
    "    last_update_epoch = -1\n",
    "\n",
    "    for i in range(EPOCH_COUNT):\n",
    "        indices = np.arange(0, train_count)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_X = train_input[indices]\n",
    "        train_Y = train_output[indices]\n",
    "\n",
    "        for j in range(0, train_count, BATCH_SIZE):\n",
    "            batch_X = train_X[j:j + BATCH_SIZE]\n",
    "            batch_Y = train_Y[j:j + BATCH_SIZE]\n",
    "\n",
    "            outputs_per_layer = forward_propagation_train(batch_X, weights, biases)\n",
    "\n",
    "            back_propagation(weights, biases, outputs_per_layer, batch_Y, learning_rate)\n",
    "\n",
    "        acc_train = test_neural_network(train_input, weights, biases, train_output)\n",
    "        acc_test = test_neural_network(test_input, weights, biases, test_output)\n",
    "        accuracies_train.append(acc_train)\n",
    "        accuracies_test.append(acc_test)\n",
    "\n",
    "        learning_rate, last_update_epoch = update_learning_rate(learning_rate, accuracies_train, i, last_update_epoch)\n",
    "\n",
    "        print(f\"{i+1}: test:{acc_test * 100: .2f}% train:{acc_train * 100: .2f}% {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a952722d-9a9d-4071-b33c-bc54e4ee68b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: test: 91.00% train: 90.68% 0.001\n",
      "2: test: 91.63% train: 91.62% 0.001\n",
      "3: test: 92.30% train: 92.35% 0.001\n",
      "4: test: 92.51% train: 92.68% 0.001\n",
      "5: test: 92.95% train: 93.08% 0.001\n",
      "6: test: 92.92% train: 93.25% 0.001\n",
      "7: test: 92.92% train: 93.19% 0.001\n",
      "8: test: 93.12% train: 93.38% 0.001\n",
      "9: test: 93.16% train: 93.45% 0.001\n",
      "10: test: 93.35% train: 93.66% 0.001\n",
      "11: test: 93.13% train: 93.69% 0.001\n",
      "12: test: 93.23% train: 93.74% 0.001\n",
      "13: test: 93.29% train: 93.74% 0.001\n",
      "14: test: 93.24% train: 93.68% 0.001\n",
      "15: test: 92.97% train: 93.49% 0.001\n",
      "16: test: 93.20% train: 93.78% 0.001\n",
      "17: test: 93.25% train: 93.69% 0.001\n",
      "18: test: 93.38% train: 93.85% 0.001\n",
      "19: test: 93.44% train: 93.92% 0.001\n",
      "20: test: 93.31% train: 93.76% 0.001\n",
      "21: test: 93.26% train: 93.85% 0.001\n",
      "22: test: 93.52% train: 93.90% 0.001\n",
      "23: test: 93.39% train: 93.94% 0.001\n",
      "24: test: 93.40% train: 93.99% 0.001\n",
      "25: test: 93.51% train: 93.94% 0.001\n",
      "26: test: 93.42% train: 93.93% 0.001\n",
      "27: test: 93.15% train: 93.78% 0.001\n",
      "28: test: 92.93% train: 93.74% 0.001\n",
      "29: test: 93.40% train: 94.10% 0.001\n",
      "30: test: 93.62% train: 94.17% 0.001\n",
      "31: test: 93.35% train: 94.02% 0.001\n",
      "32: test: 93.25% train: 93.95% 0.001\n",
      "33: test: 93.66% train: 94.21% 0.001\n",
      "34: test: 93.37% train: 94.06% 0.001\n",
      "35: test: 93.48% train: 94.01% 0.001\n",
      "36: test: 93.47% train: 94.10% 0.001\n",
      "37: test: 93.50% train: 94.09% 0.001\n",
      "38: test: 93.14% train: 93.88% 0.001\n",
      "39: test: 93.34% train: 94.00% 0.001\n",
      "Reducing learning rate from 0.001 to 0.0005\n",
      "40: test: 93.31% train: 93.97% 0.0005\n",
      "41: test: 93.61% train: 94.07% 0.0005\n",
      "42: test: 93.62% train: 94.12% 0.0005\n",
      "43: test: 93.49% train: 94.03% 0.0005\n",
      "44: test: 93.51% train: 94.15% 0.0005\n",
      "45: test: 93.62% train: 94.19% 0.0005\n",
      "46: test: 93.18% train: 93.74% 0.0005\n",
      "47: test: 93.55% train: 94.11% 0.0005\n",
      "48: test: 93.49% train: 94.17% 0.0005\n",
      "49: test: 93.48% train: 94.12% 0.0005\n",
      "50: test: 93.44% train: 94.15% 0.0005\n",
      "51: test: 93.49% train: 94.10% 0.0005\n",
      "Reducing learning rate from 0.0005 to 0.00025\n",
      "52: test: 93.55% train: 94.16% 0.00025\n",
      "53: test: 93.53% train: 94.26% 0.00025\n",
      "54: test: 93.56% train: 94.21% 0.00025\n",
      "55: test: 93.59% train: 94.26% 0.00025\n",
      "56: test: 93.52% train: 94.17% 0.00025\n",
      "57: test: 93.58% train: 94.28% 0.00025\n",
      "58: test: 93.63% train: 94.26% 0.00025\n",
      "59: test: 93.55% train: 94.27% 0.00025\n",
      "60: test: 93.60% train: 94.30% 0.00025\n",
      "61: test: 93.51% train: 94.28% 0.00025\n",
      "62: test: 93.66% train: 94.31% 0.00025\n",
      "63: test: 93.49% train: 94.28% 0.00025\n",
      "64: test: 93.43% train: 94.25% 0.00025\n",
      "65: test: 93.58% train: 94.24% 0.00025\n",
      "66: test: 93.44% train: 94.16% 0.00025\n",
      "67: test: 93.54% train: 94.27% 0.00025\n",
      "68: test: 93.62% train: 94.28% 0.00025\n",
      "Reducing learning rate from 0.00025 to 0.000125\n",
      "69: test: 93.62% train: 94.27% 0.000125\n",
      "70: test: 93.58% train: 94.32% 0.000125\n",
      "71: test: 93.56% train: 94.27% 0.000125\n",
      "72: test: 93.52% train: 94.27% 0.000125\n",
      "73: test: 93.60% train: 94.33% 0.000125\n",
      "74: test: 93.53% train: 94.27% 0.000125\n",
      "75: test: 93.64% train: 94.33% 0.000125\n",
      "76: test: 93.65% train: 94.31% 0.000125\n",
      "77: test: 93.61% train: 94.28% 0.000125\n",
      "78: test: 93.56% train: 94.25% 0.000125\n",
      "79: test: 93.46% train: 94.17% 0.000125\n",
      "80: test: 93.53% train: 94.30% 0.000125\n",
      "81: test: 93.49% train: 94.25% 0.000125\n",
      "Reducing learning rate from 0.000125 to 6.25e-05\n",
      "82: test: 93.67% train: 94.33% 6.25e-05\n",
      "83: test: 93.50% train: 94.27% 6.25e-05\n",
      "84: test: 93.63% train: 94.33% 6.25e-05\n",
      "85: test: 93.65% train: 94.32% 6.25e-05\n",
      "86: test: 93.61% train: 94.30% 6.25e-05\n",
      "87: test: 93.51% train: 94.31% 6.25e-05\n",
      "88: test: 93.52% train: 94.31% 6.25e-05\n",
      "Reducing learning rate from 6.25e-05 to 3.125e-05\n",
      "89: test: 93.51% train: 94.28% 3.125e-05\n",
      "90: test: 93.64% train: 94.32% 3.125e-05\n",
      "91: test: 93.67% train: 94.33% 3.125e-05\n",
      "92: test: 93.51% train: 94.27% 3.125e-05\n",
      "93: test: 93.58% train: 94.31% 3.125e-05\n",
      "94: test: 93.62% train: 94.33% 3.125e-05\n",
      "95: test: 93.64% train: 94.32% 3.125e-05\n",
      "96: test: 93.65% train: 94.33% 3.125e-05\n",
      "97: test: 93.68% train: 94.34% 3.125e-05\n",
      "98: test: 93.62% train: 94.30% 3.125e-05\n",
      "99: test: 93.64% train: 94.32% 3.125e-05\n",
      "100: test: 93.65% train: 94.29% 3.125e-05\n",
      "101: test: 93.62% train: 94.32% 3.125e-05\n",
      "102: test: 93.63% train: 94.31% 3.125e-05\n",
      "103: test: 93.59% train: 94.31% 3.125e-05\n",
      "Reducing learning rate from 3.125e-05 to 1.5625e-05\n",
      "104: test: 93.58% train: 94.30% 1.5625e-05\n",
      "105: test: 93.63% train: 94.31% 1.5625e-05\n",
      "106: test: 93.66% train: 94.32% 1.5625e-05\n",
      "107: test: 93.61% train: 94.32% 1.5625e-05\n",
      "108: test: 93.64% train: 94.32% 1.5625e-05\n",
      "109: test: 93.63% train: 94.32% 1.5625e-05\n",
      "110: test: 93.65% train: 94.32% 1.5625e-05\n",
      "111: test: 93.65% train: 94.33% 1.5625e-05\n",
      "112: test: 93.68% train: 94.31% 1.5625e-05\n",
      "113: test: 93.67% train: 94.32% 1.5625e-05\n",
      "114: test: 93.68% train: 94.31% 1.5625e-05\n",
      "115: test: 93.65% train: 94.32% 1.5625e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train_X, train_Y, test_X, test_Y \u001b[38;5;241m=\u001b[39m transform_initial_data(i_train_X, i_train_Y, i_test_X, i_test_Y)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_Y\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[131], line 30\u001b[0m, in \u001b[0;36mtrain_neural_network\u001b[1;34m(train_input, train_output, test_input, test_output)\u001b[0m\n\u001b[0;32m     26\u001b[0m     outputs_per_layer \u001b[38;5;241m=\u001b[39m forward_propagation_train(batch_X, weights, biases)\n\u001b[0;32m     28\u001b[0m     back_propagation(weights, biases, outputs_per_layer, batch_Y, learning_rate)\n\u001b[1;32m---> 30\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m \u001b[43mtest_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m acc_test \u001b[38;5;241m=\u001b[39m test_neural_network(test_input, weights, biases, test_output)\n\u001b[0;32m     32\u001b[0m accuracies_train\u001b[38;5;241m.\u001b[39mappend(acc_train)\n",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m, in \u001b[0;36mtest_neural_network\u001b[1;34m(test_input, weights, biases, test_output)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_neural_network\u001b[39m(test_input, weights, biases, test_output):\n\u001b[1;32m----> 8\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict(fp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     11\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39msum(predictions \u001b[38;5;241m*\u001b[39m test_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[130], line 31\u001b[0m, in \u001b[0;36mforward_propagation_test\u001b[1;34m(inputs, weights, biases)\u001b[0m\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m output_per_layer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     29\u001b[0m     z \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m w \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m---> 31\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m softmax(z)\n\u001b[0;32m     32\u001b[0m     output_per_layer\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_per_layer\n",
      "Cell \u001b[1;32mIn[119], line 16\u001b[0m, in \u001b[0;36mleaky_relu\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mleaky_relu\u001b[39m(z):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mRELU_LEAK\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_X, train_Y, test_X, test_Y = transform_initial_data(i_train_X, i_train_Y, i_test_X, i_test_Y)\n",
    "\n",
    "train_neural_network(train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680b606-2e7b-4e3d-80fc-48b51a961254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
